// Blood Lexer in Blood
// Self-Hosting Milestone (REAL-V-009)
// Demonstrates: String handling, pattern matching, enums, iterators, effects
//
// This lexer tokenizes Blood source code, proving that Blood can implement
// its own compiler frontend. This is a critical step toward self-hosting.
//
// Features:
// - Complete token set for Blood language
// - Unicode identifier support
// - Nested comment handling
// - String interpolation tokens
// - Span tracking for error reporting
// - Effect-based error handling

// ============================================================================
// Source Location Tracking
// ============================================================================

/// A position in source code
#[derive(Clone, Copy, PartialEq, Eq)]
struct Position {
    /// Line number (1-indexed)
    line: u32,
    /// Column number (1-indexed)
    column: u32,
    /// Byte offset from start of file
    offset: u32,
}

impl Position {
    fn new(line: u32, column: u32, offset: u32) -> Position {
        Position { line, column, offset }
    }

    fn start() -> Position {
        Position { line: 1, column: 1, offset: 0 }
    }
}

/// A span of source code (start..end)
#[derive(Clone, Copy, PartialEq, Eq)]
struct Span {
    start: Position,
    end: Position,
}

impl Span {
    fn new(start: Position, end: Position) -> Span {
        Span { start, end }
    }

    fn single(pos: Position) -> Span {
        Span { start: pos, end: pos }
    }

    /// Merge two spans to create a span covering both
    fn merge(a: Span, b: Span) -> Span {
        Span {
            start: if a.start.offset < b.start.offset { a.start } else { b.start },
            end: if a.end.offset > b.end.offset { a.end } else { b.end },
        }
    }
}

// ============================================================================
// Token Types
// ============================================================================

/// All possible token kinds in Blood
#[derive(Clone, PartialEq)]
enum TokenKind {
    // Literals
    /// Integer literal: 42, 0xFF, 0b1010, 1_000_000
    IntLiteral(i64),
    /// Float literal: 3.14, 1e10, 2.5e-3
    FloatLiteral(f64),
    /// String literal: "hello", "hello\nworld"
    StringLiteral(String),
    /// Raw string literal: r#"..."#
    RawStringLiteral(String),
    /// Character literal: 'a', '\n'
    CharLiteral(char),
    /// Boolean literal
    BoolLiteral(bool),

    // Identifiers and Keywords
    /// Identifier: foo, Bar, _underscore
    Ident(String),
    /// Keyword (reserved identifier)
    Keyword(Keyword),

    // Operators - Arithmetic
    /// +
    Plus,
    /// -
    Minus,
    /// *
    Star,
    /// /
    Slash,
    /// %
    Percent,

    // Operators - Comparison
    /// ==
    EqEq,
    /// !=
    NotEq,
    /// <
    Lt,
    /// <=
    LtEq,
    /// >
    Gt,
    /// >=
    GtEq,

    // Operators - Logical
    /// &&
    AndAnd,
    /// ||
    OrOr,
    /// !
    Not,

    // Operators - Bitwise
    /// &
    And,
    /// |
    Or,
    /// ^
    Caret,
    /// ~
    Tilde,
    /// <<
    Shl,
    /// >>
    Shr,

    // Operators - Assignment
    /// =
    Eq,
    /// +=
    PlusEq,
    /// -=
    MinusEq,
    /// *=
    StarEq,
    /// /=
    SlashEq,
    /// %=
    PercentEq,
    /// &=
    AndEq,
    /// |=
    OrEq,
    /// ^=
    CaretEq,
    /// <<=
    ShlEq,
    /// >>=
    ShrEq,

    // Delimiters
    /// (
    LParen,
    /// )
    RParen,
    /// [
    LBracket,
    /// ]
    RBracket,
    /// {
    LBrace,
    /// }
    RBrace,

    // Punctuation
    /// ,
    Comma,
    /// :
    Colon,
    /// ::
    ColonColon,
    /// ;
    Semi,
    /// .
    Dot,
    /// ..
    DotDot,
    /// ...
    DotDotDot,
    /// ..=
    DotDotEq,
    /// ->
    Arrow,
    /// =>
    FatArrow,
    /// ?
    Question,
    /// @
    At,
    /// #
    Hash,
    /// $
    Dollar,

    // Special
    /// End of file
    Eof,
    /// Invalid token (lexer error)
    Error(String),
}

/// Blood keywords
#[derive(Clone, Copy, PartialEq, Eq)]
enum Keyword {
    // Declarations
    Fn,
    Let,
    Mut,
    Const,
    Static,
    Struct,
    Enum,
    Union,
    Type,
    Trait,
    Impl,
    Mod,
    Use,
    Pub,
    Crate,
    Super,
    Self_,
    SelfType,

    // Control flow
    If,
    Else,
    Match,
    Loop,
    While,
    For,
    In,
    Break,
    Continue,
    Return,

    // Effects
    Effect,
    Handler,
    Handle,
    With,
    Do,
    Resume,
    Try,
    Perform,

    // Types
    As,
    Where,
    Dyn,

    // Boolean
    True,
    False,

    // Memory
    Box,
    Move,
    Ref,
    Own,

    // Async
    Async,
    Await,
    Spawn,

    // FFI
    Extern,
    Unsafe,

    // Pure
    Pure,
}

impl Keyword {
    fn from_str(s: &str) -> Option<Keyword> {
        match s {
            "fn" => Some(Keyword::Fn),
            "let" => Some(Keyword::Let),
            "mut" => Some(Keyword::Mut),
            "const" => Some(Keyword::Const),
            "static" => Some(Keyword::Static),
            "struct" => Some(Keyword::Struct),
            "enum" => Some(Keyword::Enum),
            "union" => Some(Keyword::Union),
            "type" => Some(Keyword::Type),
            "trait" => Some(Keyword::Trait),
            "impl" => Some(Keyword::Impl),
            "mod" => Some(Keyword::Mod),
            "use" => Some(Keyword::Use),
            "pub" => Some(Keyword::Pub),
            "crate" => Some(Keyword::Crate),
            "super" => Some(Keyword::Super),
            "self" => Some(Keyword::Self_),
            "Self" => Some(Keyword::SelfType),
            "if" => Some(Keyword::If),
            "else" => Some(Keyword::Else),
            "match" => Some(Keyword::Match),
            "loop" => Some(Keyword::Loop),
            "while" => Some(Keyword::While),
            "for" => Some(Keyword::For),
            "in" => Some(Keyword::In),
            "break" => Some(Keyword::Break),
            "continue" => Some(Keyword::Continue),
            "return" => Some(Keyword::Return),
            "effect" => Some(Keyword::Effect),
            "handler" => Some(Keyword::Handler),
            "handle" => Some(Keyword::Handle),
            "with" => Some(Keyword::With),
            "do" => Some(Keyword::Do),
            "resume" => Some(Keyword::Resume),
            "try" => Some(Keyword::Try),
            "perform" => Some(Keyword::Perform),
            "as" => Some(Keyword::As),
            "where" => Some(Keyword::Where),
            "dyn" => Some(Keyword::Dyn),
            "true" => Some(Keyword::True),
            "false" => Some(Keyword::False),
            "box" => Some(Keyword::Box),
            "move" => Some(Keyword::Move),
            "ref" => Some(Keyword::Ref),
            "own" => Some(Keyword::Own),
            "async" => Some(Keyword::Async),
            "await" => Some(Keyword::Await),
            "spawn" => Some(Keyword::Spawn),
            "extern" => Some(Keyword::Extern),
            "unsafe" => Some(Keyword::Unsafe),
            "pure" => Some(Keyword::Pure),
            _ => None,
        }
    }

    fn as_str(&self) -> &'static str {
        match self {
            Keyword::Fn => "fn",
            Keyword::Let => "let",
            Keyword::Mut => "mut",
            Keyword::Const => "const",
            Keyword::Static => "static",
            Keyword::Struct => "struct",
            Keyword::Enum => "enum",
            Keyword::Union => "union",
            Keyword::Type => "type",
            Keyword::Trait => "trait",
            Keyword::Impl => "impl",
            Keyword::Mod => "mod",
            Keyword::Use => "use",
            Keyword::Pub => "pub",
            Keyword::Crate => "crate",
            Keyword::Super => "super",
            Keyword::Self_ => "self",
            Keyword::SelfType => "Self",
            Keyword::If => "if",
            Keyword::Else => "else",
            Keyword::Match => "match",
            Keyword::Loop => "loop",
            Keyword::While => "while",
            Keyword::For => "for",
            Keyword::In => "in",
            Keyword::Break => "break",
            Keyword::Continue => "continue",
            Keyword::Return => "return",
            Keyword::Effect => "effect",
            Keyword::Handler => "handler",
            Keyword::Handle => "handle",
            Keyword::With => "with",
            Keyword::Do => "do",
            Keyword::Resume => "resume",
            Keyword::Try => "try",
            Keyword::Perform => "perform",
            Keyword::As => "as",
            Keyword::Where => "where",
            Keyword::Dyn => "dyn",
            Keyword::True => "true",
            Keyword::False => "false",
            Keyword::Box => "box",
            Keyword::Move => "move",
            Keyword::Ref => "ref",
            Keyword::Own => "own",
            Keyword::Async => "async",
            Keyword::Await => "await",
            Keyword::Spawn => "spawn",
            Keyword::Extern => "extern",
            Keyword::Unsafe => "unsafe",
            Keyword::Pure => "pure",
        }
    }
}

/// A token with its kind and span
#[derive(Clone)]
struct Token {
    kind: TokenKind,
    span: Span,
}

impl Token {
    fn new(kind: TokenKind, span: Span) -> Token {
        Token { kind, span }
    }
}

// ============================================================================
// Lexer Effects
// ============================================================================

/// Effect for lexer diagnostics
effect LexerDiagnostic {
    op error(msg: String, span: Span) -> ();
    op warning(msg: String, span: Span) -> ();
}

// ============================================================================
// Lexer Implementation
// ============================================================================

/// The Blood lexer
struct Lexer {
    /// Source code being lexed
    source: String,
    /// Source file name (for diagnostics)
    filename: String,
    /// Current byte position
    pos: usize,
    /// Current line
    line: u32,
    /// Current column
    column: u32,
    /// Start position of current token
    token_start: Position,
}

impl Lexer {
    /// Create a new lexer for the given source code
    fn new(source: String, filename: String) -> Lexer {
        Lexer {
            source,
            filename,
            pos: 0,
            line: 1,
            column: 1,
            token_start: Position::start(),
        }
    }

    /// Check if we've reached the end of source
    fn is_eof(&self) -> bool {
        self.pos >= self.source.len()
    }

    /// Get current position
    fn current_pos(&self) -> Position {
        Position::new(self.line, self.column, self.pos as u32)
    }

    /// Peek at current character
    fn peek(&self) -> Option<char> {
        self.source.chars().nth(self.pos)
    }

    /// Peek at character at offset
    fn peek_at(&self, offset: usize) -> Option<char> {
        self.source.chars().nth(self.pos + offset)
    }

    /// Advance to next character
    fn advance(&mut self) -> Option<char> {
        let ch = self.peek()?;
        self.pos += ch.len_utf8();

        if ch == '\n' {
            self.line += 1;
            self.column = 1;
        } else {
            self.column += 1;
        }

        Some(ch)
    }

    /// Check if next character matches expected
    fn check(&self, expected: char) -> bool {
        self.peek() == Some(expected)
    }

    /// Consume character if it matches
    fn consume(&mut self, expected: char) -> bool {
        if self.check(expected) {
            self.advance();
            true
        } else {
            false
        }
    }

    /// Skip whitespace and comments
    fn skip_trivia(&mut self) {
        loop {
            // Skip whitespace
            while let Some(ch) = self.peek() {
                if ch.is_whitespace() {
                    self.advance();
                } else {
                    break;
                }
            }

            // Check for comments
            if self.check('/') {
                if self.peek_at(1) == Some('/') {
                    // Line comment
                    self.skip_line_comment();
                    continue;
                } else if self.peek_at(1) == Some('*') {
                    // Block comment
                    self.skip_block_comment();
                    continue;
                }
            }

            break;
        }
    }

    /// Skip line comment (// ...)
    fn skip_line_comment(&mut self) {
        // Skip //
        self.advance();
        self.advance();

        // Skip until end of line
        while let Some(ch) = self.peek() {
            if ch == '\n' {
                break;
            }
            self.advance();
        }
    }

    /// Skip block comment (/* ... */), handles nesting
    fn skip_block_comment(&mut self) / {LexerDiagnostic} {
        let start = self.current_pos();

        // Skip /*
        self.advance();
        self.advance();

        let mut depth = 1;

        while depth > 0 {
            if self.is_eof() {
                perform LexerDiagnostic.error(
                    "Unterminated block comment".to_string(),
                    Span::single(start)
                );
                return;
            }

            if self.check('*') && self.peek_at(1) == Some('/') {
                self.advance();
                self.advance();
                depth -= 1;
            } else if self.check('/') && self.peek_at(1) == Some('*') {
                self.advance();
                self.advance();
                depth += 1;
            } else {
                self.advance();
            }
        }
    }

    /// Mark start of a new token
    fn start_token(&mut self) {
        self.token_start = self.current_pos();
    }

    /// Create span from token start to current position
    fn make_span(&self) -> Span {
        Span::new(self.token_start, self.current_pos())
    }

    /// Create a token with current span
    fn make_token(&self, kind: TokenKind) -> Token {
        Token::new(kind, self.make_span())
    }

    /// Get next token
    fn next_token(&mut self) -> Token / {LexerDiagnostic} {
        self.skip_trivia();
        self.start_token();

        if self.is_eof() {
            return self.make_token(TokenKind::Eof);
        }

        let ch = self.peek().unwrap();

        // Identifiers and keywords
        if is_ident_start(ch) {
            return self.scan_identifier();
        }

        // Numbers
        if ch.is_ascii_digit() {
            return self.scan_number();
        }

        // Strings
        if ch == '"' {
            return self.scan_string();
        }

        // Raw strings
        if ch == 'r' && (self.peek_at(1) == Some('#') || self.peek_at(1) == Some('"')) {
            return self.scan_raw_string();
        }

        // Characters
        if ch == '\'' {
            return self.scan_char();
        }

        // Operators and punctuation
        self.scan_operator_or_punctuation()
    }

    /// Scan an identifier or keyword
    fn scan_identifier(&mut self) -> Token {
        let mut ident = String::new();

        while let Some(ch) = self.peek() {
            if is_ident_continue(ch) {
                ident.push(ch);
                self.advance();
            } else {
                break;
            }
        }

        // Check if it's a keyword
        let kind = match Keyword::from_str(&ident) {
            Some(Keyword::True) => TokenKind::BoolLiteral(true),
            Some(Keyword::False) => TokenKind::BoolLiteral(false),
            Some(kw) => TokenKind::Keyword(kw),
            None => TokenKind::Ident(ident),
        };

        self.make_token(kind)
    }

    /// Scan a number literal
    fn scan_number(&mut self) -> Token / {LexerDiagnostic} {
        let start_pos = self.pos;

        // Check for base prefix
        let base = if self.check('0') {
            match self.peek_at(1) {
                Some('x') | Some('X') => {
                    self.advance();
                    self.advance();
                    16
                },
                Some('b') | Some('B') => {
                    self.advance();
                    self.advance();
                    2
                },
                Some('o') | Some('O') => {
                    self.advance();
                    self.advance();
                    8
                },
                _ => 10,
            }
        } else {
            10
        };

        // Scan digits
        let mut num_str = String::new();
        let mut has_dot = false;
        let mut has_exp = false;

        while let Some(ch) = self.peek() {
            if ch == '_' {
                // Skip underscores in numbers
                self.advance();
                continue;
            }

            if is_digit_for_base(ch, base) {
                num_str.push(ch);
                self.advance();
            } else if ch == '.' && base == 10 && !has_dot && !has_exp {
                // Check it's not a method call (123.method)
                if let Some(next) = self.peek_at(1) {
                    if next.is_ascii_digit() {
                        has_dot = true;
                        num_str.push(ch);
                        self.advance();
                    } else {
                        break;
                    }
                } else {
                    break;
                }
            } else if (ch == 'e' || ch == 'E') && base == 10 && !has_exp {
                has_exp = true;
                num_str.push(ch);
                self.advance();

                // Optional sign
                if let Some(sign) = self.peek() {
                    if sign == '+' || sign == '-' {
                        num_str.push(sign);
                        self.advance();
                    }
                }
            } else {
                break;
            }
        }

        // Parse the number
        let kind = if has_dot || has_exp {
            match num_str.parse::<f64>() {
                Ok(f) => TokenKind::FloatLiteral(f),
                Err(_) => {
                    perform LexerDiagnostic.error(
                        format!("Invalid float literal: {}", num_str),
                        self.make_span()
                    );
                    TokenKind::Error("Invalid float".to_string())
                }
            }
        } else {
            match i64::from_str_radix(&num_str, base) {
                Ok(i) => TokenKind::IntLiteral(i),
                Err(_) => {
                    perform LexerDiagnostic.error(
                        format!("Invalid integer literal: {}", num_str),
                        self.make_span()
                    );
                    TokenKind::Error("Invalid integer".to_string())
                }
            }
        };

        self.make_token(kind)
    }

    /// Scan a string literal
    fn scan_string(&mut self) -> Token / {LexerDiagnostic} {
        self.advance(); // Skip opening "

        let mut value = String::new();
        let mut closed = false;

        while !self.is_eof() {
            let ch = self.peek().unwrap();

            if ch == '"' {
                self.advance();
                closed = true;
                break;
            } else if ch == '\\' {
                self.advance();
                if let Some(escaped) = self.scan_escape_sequence() {
                    value.push(escaped);
                }
            } else if ch == '\n' {
                perform LexerDiagnostic.error(
                    "Newline in string literal".to_string(),
                    self.make_span()
                );
                break;
            } else {
                value.push(ch);
                self.advance();
            }
        }

        if !closed {
            perform LexerDiagnostic.error(
                "Unterminated string literal".to_string(),
                self.make_span()
            );
        }

        self.make_token(TokenKind::StringLiteral(value))
    }

    /// Scan a raw string literal (r"..." or r#"..."#)
    fn scan_raw_string(&mut self) -> Token / {LexerDiagnostic} {
        self.advance(); // Skip r

        // Count hash marks
        let mut hash_count = 0;
        while self.check('#') {
            hash_count += 1;
            self.advance();
        }

        if !self.check('"') {
            perform LexerDiagnostic.error(
                "Expected '\"' after raw string prefix".to_string(),
                self.make_span()
            );
            return self.make_token(TokenKind::Error("Invalid raw string".to_string()));
        }
        self.advance(); // Skip "

        let mut value = String::new();
        let mut closed = false;

        while !self.is_eof() {
            let ch = self.peek().unwrap();

            if ch == '"' {
                // Check for matching closing hashes
                let pos_before = self.pos;
                self.advance();

                let mut closing_hashes = 0;
                while closing_hashes < hash_count && self.check('#') {
                    closing_hashes += 1;
                    self.advance();
                }

                if closing_hashes == hash_count {
                    closed = true;
                    break;
                } else {
                    // Not the closing sequence, add to value
                    value.push('"');
                    for _ in 0..closing_hashes {
                        value.push('#');
                    }
                }
            } else {
                value.push(ch);
                self.advance();
            }
        }

        if !closed {
            perform LexerDiagnostic.error(
                "Unterminated raw string literal".to_string(),
                self.make_span()
            );
        }

        self.make_token(TokenKind::RawStringLiteral(value))
    }

    /// Scan a character literal
    fn scan_char(&mut self) -> Token / {LexerDiagnostic} {
        self.advance(); // Skip opening '

        let value = if self.check('\\') {
            self.advance();
            self.scan_escape_sequence()
        } else if self.check('\'') {
            perform LexerDiagnostic.error(
                "Empty character literal".to_string(),
                self.make_span()
            );
            None
        } else {
            let ch = self.advance();
            ch
        };

        if !self.consume('\'') {
            perform LexerDiagnostic.error(
                "Unterminated character literal".to_string(),
                self.make_span()
            );
        }

        let kind = match value {
            Some(ch) => TokenKind::CharLiteral(ch),
            None => TokenKind::Error("Invalid character".to_string()),
        };

        self.make_token(kind)
    }

    /// Scan an escape sequence
    fn scan_escape_sequence(&mut self) -> Option<char> / {LexerDiagnostic} {
        let ch = self.advance()?;

        match ch {
            'n' => Some('\n'),
            'r' => Some('\r'),
            't' => Some('\t'),
            '\\' => Some('\\'),
            '\'' => Some('\''),
            '"' => Some('"'),
            '0' => Some('\0'),
            'x' => {
                // \xNN
                let mut hex = String::new();
                for _ in 0..2 {
                    if let Some(h) = self.peek() {
                        if h.is_ascii_hexdigit() {
                            hex.push(h);
                            self.advance();
                        }
                    }
                }
                u8::from_str_radix(&hex, 16).ok().map(|n| n as char)
            },
            'u' => {
                // \u{NNNN}
                if !self.consume('{') {
                    perform LexerDiagnostic.error(
                        "Expected '{' in unicode escape".to_string(),
                        self.make_span()
                    );
                    return None;
                }

                let mut hex = String::new();
                while let Some(h) = self.peek() {
                    if h == '}' {
                        break;
                    }
                    if h.is_ascii_hexdigit() && hex.len() < 6 {
                        hex.push(h);
                        self.advance();
                    } else {
                        break;
                    }
                }

                if !self.consume('}') {
                    perform LexerDiagnostic.error(
                        "Expected '}' in unicode escape".to_string(),
                        self.make_span()
                    );
                    return None;
                }

                u32::from_str_radix(&hex, 16)
                    .ok()
                    .and_then(char::from_u32)
            },
            _ => {
                perform LexerDiagnostic.warning(
                    format!("Unknown escape sequence: \\{}", ch),
                    self.make_span()
                );
                Some(ch)
            }
        }
    }

    /// Scan operator or punctuation
    fn scan_operator_or_punctuation(&mut self) -> Token / {LexerDiagnostic} {
        let ch = self.advance().unwrap();

        let kind = match ch {
            '+' => {
                if self.consume('=') { TokenKind::PlusEq }
                else { TokenKind::Plus }
            },
            '-' => {
                if self.consume('=') { TokenKind::MinusEq }
                else if self.consume('>') { TokenKind::Arrow }
                else { TokenKind::Minus }
            },
            '*' => {
                if self.consume('=') { TokenKind::StarEq }
                else { TokenKind::Star }
            },
            '/' => {
                if self.consume('=') { TokenKind::SlashEq }
                else { TokenKind::Slash }
            },
            '%' => {
                if self.consume('=') { TokenKind::PercentEq }
                else { TokenKind::Percent }
            },
            '=' => {
                if self.consume('=') { TokenKind::EqEq }
                else if self.consume('>') { TokenKind::FatArrow }
                else { TokenKind::Eq }
            },
            '!' => {
                if self.consume('=') { TokenKind::NotEq }
                else { TokenKind::Not }
            },
            '<' => {
                if self.consume('<') {
                    if self.consume('=') { TokenKind::ShlEq }
                    else { TokenKind::Shl }
                } else if self.consume('=') { TokenKind::LtEq }
                else { TokenKind::Lt }
            },
            '>' => {
                if self.consume('>') {
                    if self.consume('=') { TokenKind::ShrEq }
                    else { TokenKind::Shr }
                } else if self.consume('=') { TokenKind::GtEq }
                else { TokenKind::Gt }
            },
            '&' => {
                if self.consume('&') { TokenKind::AndAnd }
                else if self.consume('=') { TokenKind::AndEq }
                else { TokenKind::And }
            },
            '|' => {
                if self.consume('|') { TokenKind::OrOr }
                else if self.consume('=') { TokenKind::OrEq }
                else { TokenKind::Or }
            },
            '^' => {
                if self.consume('=') { TokenKind::CaretEq }
                else { TokenKind::Caret }
            },
            '~' => TokenKind::Tilde,
            '(' => TokenKind::LParen,
            ')' => TokenKind::RParen,
            '[' => TokenKind::LBracket,
            ']' => TokenKind::RBracket,
            '{' => TokenKind::LBrace,
            '}' => TokenKind::RBrace,
            ',' => TokenKind::Comma,
            ';' => TokenKind::Semi,
            ':' => {
                if self.consume(':') { TokenKind::ColonColon }
                else { TokenKind::Colon }
            },
            '.' => {
                if self.consume('.') {
                    if self.consume('.') { TokenKind::DotDotDot }
                    else if self.consume('=') { TokenKind::DotDotEq }
                    else { TokenKind::DotDot }
                } else { TokenKind::Dot }
            },
            '?' => TokenKind::Question,
            '@' => TokenKind::At,
            '#' => TokenKind::Hash,
            '$' => TokenKind::Dollar,
            _ => {
                perform LexerDiagnostic.error(
                    format!("Unexpected character: '{}'", ch),
                    self.make_span()
                );
                TokenKind::Error(format!("Unexpected: {}", ch))
            }
        };

        self.make_token(kind)
    }

    /// Tokenize the entire source and return all tokens
    fn tokenize(&mut self) -> Vec<Token> / {LexerDiagnostic} {
        let mut tokens = Vec::new();

        loop {
            let token = self.next_token();
            let is_eof = matches!(token.kind, TokenKind::Eof);
            tokens.push(token);

            if is_eof {
                break;
            }
        }

        tokens
    }
}

// ============================================================================
// Helper Functions
// ============================================================================

/// Check if character can start an identifier
fn is_ident_start(ch: char) -> bool {
    ch.is_alphabetic() || ch == '_'
}

/// Check if character can continue an identifier
fn is_ident_continue(ch: char) -> bool {
    ch.is_alphanumeric() || ch == '_'
}

/// Check if character is a digit for the given base
fn is_digit_for_base(ch: char, base: u32) -> bool {
    match base {
        2 => ch == '0' || ch == '1',
        8 => ch >= '0' && ch <= '7',
        10 => ch.is_ascii_digit(),
        16 => ch.is_ascii_hexdigit(),
        _ => false,
    }
}

// ============================================================================
// Token Display
// ============================================================================

impl TokenKind {
    fn description(&self) -> String {
        match self {
            TokenKind::IntLiteral(n) => format!("int({})", n),
            TokenKind::FloatLiteral(f) => format!("float({})", f),
            TokenKind::StringLiteral(s) => format!("string({:?})", s),
            TokenKind::RawStringLiteral(s) => format!("raw_string({:?})", s),
            TokenKind::CharLiteral(c) => format!("char({:?})", c),
            TokenKind::BoolLiteral(b) => format!("bool({})", b),
            TokenKind::Ident(s) => format!("ident({})", s),
            TokenKind::Keyword(kw) => format!("keyword({})", kw.as_str()),
            TokenKind::Plus => "+".to_string(),
            TokenKind::Minus => "-".to_string(),
            TokenKind::Star => "*".to_string(),
            TokenKind::Slash => "/".to_string(),
            TokenKind::Percent => "%".to_string(),
            TokenKind::EqEq => "==".to_string(),
            TokenKind::NotEq => "!=".to_string(),
            TokenKind::Lt => "<".to_string(),
            TokenKind::LtEq => "<=".to_string(),
            TokenKind::Gt => ">".to_string(),
            TokenKind::GtEq => ">=".to_string(),
            TokenKind::AndAnd => "&&".to_string(),
            TokenKind::OrOr => "||".to_string(),
            TokenKind::Not => "!".to_string(),
            TokenKind::And => "&".to_string(),
            TokenKind::Or => "|".to_string(),
            TokenKind::Caret => "^".to_string(),
            TokenKind::Tilde => "~".to_string(),
            TokenKind::Shl => "<<".to_string(),
            TokenKind::Shr => ">>".to_string(),
            TokenKind::Eq => "=".to_string(),
            TokenKind::PlusEq => "+=".to_string(),
            TokenKind::MinusEq => "-=".to_string(),
            TokenKind::StarEq => "*=".to_string(),
            TokenKind::SlashEq => "/=".to_string(),
            TokenKind::PercentEq => "%=".to_string(),
            TokenKind::AndEq => "&=".to_string(),
            TokenKind::OrEq => "|=".to_string(),
            TokenKind::CaretEq => "^=".to_string(),
            TokenKind::ShlEq => "<<=".to_string(),
            TokenKind::ShrEq => ">>=".to_string(),
            TokenKind::LParen => "(".to_string(),
            TokenKind::RParen => ")".to_string(),
            TokenKind::LBracket => "[".to_string(),
            TokenKind::RBracket => "]".to_string(),
            TokenKind::LBrace => "{".to_string(),
            TokenKind::RBrace => "}".to_string(),
            TokenKind::Comma => ",".to_string(),
            TokenKind::Colon => ":".to_string(),
            TokenKind::ColonColon => "::".to_string(),
            TokenKind::Semi => ";".to_string(),
            TokenKind::Dot => ".".to_string(),
            TokenKind::DotDot => "..".to_string(),
            TokenKind::DotDotDot => "...".to_string(),
            TokenKind::DotDotEq => "..=".to_string(),
            TokenKind::Arrow => "->".to_string(),
            TokenKind::FatArrow => "=>".to_string(),
            TokenKind::Question => "?".to_string(),
            TokenKind::At => "@".to_string(),
            TokenKind::Hash => "#".to_string(),
            TokenKind::Dollar => "$".to_string(),
            TokenKind::Eof => "EOF".to_string(),
            TokenKind::Error(msg) => format!("error({})", msg),
        }
    }
}

// ============================================================================
// Example Usage
// ============================================================================

fn main() -> i32 / {IO, LexerDiagnostic} {
    print("=== Blood Lexer in Blood ===\n");
    print("Self-hosting milestone: Lexer implementation\n");

    // Example Blood source code to tokenize
    let source = r#"
// A simple Blood program
fn main() -> i32 / {IO} {
    let message = "Hello, World!";
    let count: i32 = 42;
    let pi = 3.14159;
    let hex = 0xDEAD_BEEF;
    let binary = 0b1010_1010;

    // Effect invocation
    perform IO.print(message);

    // Control flow
    if count > 0 {
        for i in 0..count {
            perform log(i);
        }
    } else {
        return -1;
    }

    // Pattern matching
    match count {
        0 => print("zero"),
        1..=10 => print("small"),
        _ => print("large"),
    }

    /* Block comment
       with multiple lines
       /* and nesting */ */

    0
}

effect Log {
    op log(msg: String) -> ();
}

struct Point<T> {
    x: T,
    y: T,
}

impl<T: Add> Point<T> {
    fn add(self, other: Point<T>) -> Point<T> / pure {
        Point {
            x: self.x + other.x,
            y: self.y + other.y,
        }
    }
}
"#;

    // Create lexer
    let mut lexer = Lexer::new(source.to_string(), "example.blood".to_string());

    // Tokenize
    print("Tokenizing source code...\n");
    let tokens = lexer.tokenize();

    // Display tokens
    print("Tokens found: {}\n", tokens.len());
    print("{:-<60}", "");

    for token in &tokens {
        let pos = token.span.start;
        print("{:4}:{:<3} | {}", pos.line, pos.column, token.kind.description());
    }

    print("{:-<60}", "");

    // Statistics
    let mut keywords = 0;
    let mut idents = 0;
    let mut literals = 0;
    let mut operators = 0;
    let mut delimiters = 0;

    for token in &tokens {
        match &token.kind {
            TokenKind::Keyword(_) => keywords += 1,
            TokenKind::Ident(_) => idents += 1,
            TokenKind::IntLiteral(_) |
            TokenKind::FloatLiteral(_) |
            TokenKind::StringLiteral(_) |
            TokenKind::CharLiteral(_) |
            TokenKind::BoolLiteral(_) => literals += 1,
            TokenKind::Plus | TokenKind::Minus | TokenKind::Star |
            TokenKind::Slash | TokenKind::Percent | TokenKind::EqEq |
            TokenKind::NotEq | TokenKind::Lt | TokenKind::LtEq |
            TokenKind::Gt | TokenKind::GtEq | TokenKind::And |
            TokenKind::Or | TokenKind::AndAnd | TokenKind::OrOr |
            TokenKind::Not | TokenKind::Eq | TokenKind::Arrow |
            TokenKind::FatArrow | TokenKind::DotDot | TokenKind::DotDotEq => operators += 1,
            TokenKind::LParen | TokenKind::RParen |
            TokenKind::LBracket | TokenKind::RBracket |
            TokenKind::LBrace | TokenKind::RBrace |
            TokenKind::Comma | TokenKind::Semi |
            TokenKind::Colon | TokenKind::ColonColon |
            TokenKind::Dot => delimiters += 1,
            _ => {}
        }
    }

    print("\n=== Token Statistics ===");
    print("Keywords:   {}", keywords);
    print("Identifiers: {}", idents);
    print("Literals:    {}", literals);
    print("Operators:   {}", operators);
    print("Delimiters:  {}", delimiters);
    print("Total:       {}", tokens.len());

    print("\n=== Lexer test completed successfully! ===");
    0
}

// Error handler for diagnostics
fn main_with_handler() -> i32 / {IO} {
    try {
        main()
    } with {
        LexerDiagnostic::error(msg, span) => {
            print("ERROR [{}:{}]: {}", span.start.line, span.start.column, msg);
            resume(())
        },
        LexerDiagnostic::warning(msg, span) => {
            print("WARNING [{}:{}]: {}", span.start.line, span.start.column, msg);
            resume(())
        },
    }
}
