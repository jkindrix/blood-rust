// Concurrent Web Scraper in Blood
// A concurrent web scraper demonstrating Blood's fiber-based concurrency,
// HTTP client, HTML parsing, and effect-based I/O.
//
// This example shows:
// - Fiber-based concurrency with channels
// - HTTP client usage
// - Simple HTML parsing
// - Effect composition (Net + Async + IO)
// - Work queue pattern
// - Rate limiting
// - Error handling in concurrent context

// ===========================================================================
// 1. HTML Parsing (Simple Link Extractor)
// ===========================================================================

/// Represents an extracted link from HTML
struct Link {
    url: String,
    text: String,
    rel: Option<String>,
}

impl Link {
    fn new(url: &str, text: &str) -> Link / pure {
        Link {
            url: url.to_string(),
            text: text.to_string(),
            rel: None,
        }
    }

    fn with_rel(url: &str, text: &str, rel: &str) -> Link / pure {
        Link {
            url: url.to_string(),
            text: text.to_string(),
            rel: Some(rel.to_string()),
        }
    }

    /// Check if this is a relative URL
    fn is_relative(&self) -> bool / pure {
        !self.url.starts_with("http://") && !self.url.starts_with("https://")
    }

    /// Resolve a relative URL against a base URL
    fn resolve(&self, base: &str) -> String / pure {
        if !self.is_relative() {
            return self.url.clone();
        }

        // Extract base URL parts
        let scheme_end = base.find("://").unwrap_or(0) + 3;
        let host_end = base[scheme_end..].find('/').map(|i| scheme_end + i).unwrap_or(base.len());
        let base_origin = &base[..host_end];

        if self.url.starts_with('/') {
            // Absolute path relative to host
            format!("{}{}", base_origin, self.url)
        } else if self.url.starts_with('#') || self.url.starts_with('?') {
            // Fragment or query - append to current URL
            format!("{}{}", base, self.url)
        } else {
            // Relative path - resolve against current directory
            let last_slash = base.rfind('/').unwrap_or(host_end);
            let base_path = &base[..last_slash + 1];
            format!("{}{}", base_path, self.url)
        }
    }
}

/// Simple HTML link extractor (not a full parser)
struct LinkExtractor {
    base_url: String,
}

impl LinkExtractor {
    fn new(base_url: &str) -> LinkExtractor / pure {
        LinkExtractor {
            base_url: base_url.to_string(),
        }
    }

    /// Extract all links from HTML content
    fn extract(&self, html: &str) -> Vec<Link> / pure {
        let mut links = Vec::new();
        let mut pos = 0;

        while pos < html.len() {
            // Find <a tags
            if let Some(a_start) = html[pos..].find("<a ") {
                let tag_start = pos + a_start;

                // Find end of tag
                if let Some(tag_end_rel) = html[tag_start..].find('>') {
                    let tag_end = tag_start + tag_end_rel;
                    let tag_content = &html[tag_start..tag_end + 1];

                    // Extract href attribute
                    if let Some(href) = self.extract_attribute(tag_content, "href") {
                        // Find link text (content between <a> and </a>)
                        let text_start = tag_end + 1;
                        let text_end = html[text_start..].find("</a>")
                            .map(|i| text_start + i)
                            .unwrap_or(text_start);
                        let text = self.strip_tags(&html[text_start..text_end]).trim().to_string();

                        // Extract rel attribute if present
                        let rel = self.extract_attribute(tag_content, "rel");

                        let link = match rel {
                            Some(r) => Link::with_rel(&href, &text, &r),
                            None => Link::new(&href, &text),
                        };

                        // Resolve relative URLs
                        let resolved_url = link.resolve(&self.base_url);
                        links.push(Link {
                            url: resolved_url,
                            text: link.text,
                            rel: link.rel,
                        });
                    }

                    pos = tag_end + 1;
                } else {
                    pos += a_start + 1;
                }
            } else {
                break;
            }
        }

        links
    }

    /// Extract an attribute value from a tag
    fn extract_attribute(&self, tag: &str, attr: &str) -> Option<String> / pure {
        let patterns = [
            format!(r#"{}=""#, attr),
            format!(r#"{}='"#, attr),
            format!(r#"{}="#, attr),
        ];

        for pattern in &patterns {
            if let Some(start) = tag.find(pattern.as_str()) {
                let value_start = start + pattern.len();
                let quote = if pattern.ends_with('"') {
                    '"'
                } else if pattern.ends_with('\'') {
                    '\''
                } else {
                    ' ' // Unquoted attribute
                };

                let value_end = html[value_start..].find(quote)
                    .or_else(|| html[value_start..].find('>'))
                    .map(|i| value_start + i)
                    .unwrap_or(tag.len());

                return Some(tag[value_start..value_end].to_string());
            }
        }
        None
    }

    /// Strip HTML tags from text
    fn strip_tags(&self, html: &str) -> String / pure {
        let mut result = String::new();
        let mut in_tag = false;

        for c in html.chars() {
            match c {
                '<' => in_tag = true,
                '>' => in_tag = false,
                _ if !in_tag => result.push(c),
                _ => {}
            }
        }

        result
    }
}

// ===========================================================================
// 2. Scraper Types
// ===========================================================================

/// Scraper configuration
struct ScraperConfig {
    max_depth: u32,
    max_pages: u32,
    concurrency: u32,
    rate_limit_ms: u32,
    user_agent: String,
    allowed_domains: Option<Vec<String>>,
    timeout_ms: u32,
}

impl ScraperConfig {
    fn default() -> ScraperConfig / pure {
        ScraperConfig {
            max_depth: 3,
            max_pages: 100,
            concurrency: 5,
            rate_limit_ms: 1000,
            user_agent: "Blood-WebScraper/1.0".to_string(),
            allowed_domains: None,
            timeout_ms: 30000,
        }
    }
}

/// A page to be scraped
struct ScrapePage {
    url: String,
    depth: u32,
    parent: Option<String>,
}

/// Result of scraping a page
struct ScrapeResult {
    url: String,
    status_code: u16,
    title: Option<String>,
    links: Vec<Link>,
    depth: u32,
    error: Option<String>,
}

impl ScrapeResult {
    fn success(url: &str, status_code: u16, title: Option<String>, links: Vec<Link>, depth: u32) -> ScrapeResult / pure {
        ScrapeResult {
            url: url.to_string(),
            status_code,
            title,
            links,
            depth,
            error: None,
        }
    }

    fn failure(url: &str, error: &str, depth: u32) -> ScrapeResult / pure {
        ScrapeResult {
            url: url.to_string(),
            status_code: 0,
            title: None,
            links: Vec::new(),
            depth,
            error: Some(error.to_string()),
        }
    }
}

/// Scraper statistics
struct ScrapeStats {
    pages_scraped: u32,
    pages_failed: u32,
    links_found: u32,
    bytes_downloaded: u64,
    start_time: u64,
    end_time: u64,
}

impl ScrapeStats {
    fn new() -> ScrapeStats / pure {
        ScrapeStats {
            pages_scraped: 0,
            pages_failed: 0,
            links_found: 0,
            bytes_downloaded: 0,
            start_time: 0,
            end_time: 0,
        }
    }

    fn duration_secs(&self) -> f64 / pure {
        ((self.end_time - self.start_time) as f64) / 1000.0
    }

    fn pages_per_second(&self) -> f64 / pure {
        let duration = self.duration_secs();
        if duration > 0.0 {
            (self.pages_scraped as f64) / duration
        } else {
            0.0
        }
    }
}

// ===========================================================================
// 3. Effect Definitions
// ===========================================================================

/// Effect for scraping operations
effect Scrape {
    /// Called when a page is discovered
    op on_page_discovered(url: &str, depth: u32);

    /// Called when a page is scraped
    op on_page_scraped(result: &ScrapeResult);

    /// Check if URL should be scraped
    op should_scrape(url: &str) -> bool;
}

// ===========================================================================
// 4. Concurrent Web Scraper
// ===========================================================================

/// Main web scraper using fibers for concurrency
struct WebScraper {
    config: ScraperConfig,
    http_client: HttpClient,
}

impl WebScraper {
    fn new(config: ScraperConfig) -> WebScraper / pure {
        let http_config = HttpClientConfig {
            timeout_ms: config.timeout_ms,
            max_redirects: 5,
            follow_redirects: true,
            user_agent: config.user_agent.clone(),
        };

        WebScraper {
            config,
            http_client: HttpClient::with_config(http_config),
        }
    }

    /// Start scraping from a seed URL
    fn scrape(&self, seed_url: &str) -> Vec<ScrapeResult> / {Net, Async, IO} {
        let mut results = Vec::new();
        let mut visited = HashSet::new();
        let mut queue: Vec<ScrapePage> = Vec::new();

        // Add seed URL to queue
        queue.push(ScrapePage {
            url: seed_url.to_string(),
            depth: 0,
            parent: None,
        });

        let mut pages_scraped = 0;

        while !queue.is_empty() && pages_scraped < self.config.max_pages {
            // Take batch of URLs for concurrent processing
            let batch_size = self.config.concurrency.min(queue.len() as u32) as usize;
            let batch: Vec<ScrapePage> = queue.drain(..batch_size).collect();

            // Process batch concurrently using fibers
            let batch_results = self.process_batch(&batch, &mut visited);

            for result in batch_results {
                // Add discovered links to queue
                if result.error.is_none() && result.depth < self.config.max_depth {
                    for link in &result.links {
                        if !visited.contains(&link.url) && self.should_follow(&link.url) {
                            visited.insert(link.url.clone());
                            queue.push(ScrapePage {
                                url: link.url.clone(),
                                depth: result.depth + 1,
                                parent: Some(result.url.clone()),
                            });
                        }
                    }
                }

                results.push(result);
                pages_scraped += 1;
            }

            // Rate limiting
            if self.config.rate_limit_ms > 0 {
                perform Async.sleep(Duration::from_millis(self.config.rate_limit_ms as u64));
            }
        }

        results
    }

    /// Process a batch of pages concurrently
    fn process_batch(&self, batch: &[ScrapePage], visited: &mut HashSet<String>) -> Vec<ScrapeResult> / {Net, Async} {
        // Create channels for results
        let (tx, rx) = channel::<ScrapeResult>();

        // Spawn fiber for each page
        let mut handles = Vec::new();
        for page in batch {
            if visited.contains(&page.url) {
                continue;
            }
            visited.insert(page.url.clone());

            let url = page.url.clone();
            let depth = page.depth;
            let tx = tx.clone();
            let client = &self.http_client;

            let handle = fiber::spawn(move || {
                let result = self.scrape_page(&url, depth);
                tx.send(result);
            });
            handles.push(handle);
        }

        // Collect results
        let mut results = Vec::new();
        for _ in 0..handles.len() {
            if let Ok(result) = rx.recv_timeout(Duration::from_millis(self.config.timeout_ms as u64)) {
                results.push(result);
            }
        }

        // Wait for all fibers to complete
        for handle in handles {
            handle.join();
        }

        results
    }

    /// Scrape a single page
    fn scrape_page(&self, url: &str, depth: u32) -> ScrapeResult / {Net} {
        match self.http_client.get(url) {
            Ok(response) => {
                if !response.is_success() {
                    return ScrapeResult::failure(
                        url,
                        &format!("HTTP {}", response.status_code),
                        depth
                    );
                }

                match response.body_text() {
                    Ok(html) => {
                        let title = self.extract_title(&html);
                        let extractor = LinkExtractor::new(url);
                        let links = extractor.extract(&html);

                        ScrapeResult::success(url, response.status_code, title, links, depth)
                    }
                    Err(_) => {
                        ScrapeResult::failure(url, "Failed to decode body", depth)
                    }
                }
            }
            Err(e) => {
                ScrapeResult::failure(url, &e.to_string(), depth)
            }
        }
    }

    /// Extract page title from HTML
    fn extract_title(&self, html: &str) -> Option<String> / pure {
        let title_start = html.find("<title>")?;
        let title_end = html[title_start..].find("</title>")?;
        let title = &html[title_start + 7..title_start + title_end];
        Some(title.trim().to_string())
    }

    /// Check if URL should be followed
    fn should_follow(&self, url: &str) -> bool / pure {
        // Must be HTTP(S)
        if !url.starts_with("http://") && !url.starts_with("https://") {
            return false;
        }

        // Check allowed domains if configured
        if let Some(ref allowed) = self.config.allowed_domains {
            let url_lower = url.to_lowercase();
            let mut domain_allowed = false;
            for domain in allowed {
                if url_lower.contains(&domain.to_lowercase()) {
                    domain_allowed = true;
                    break;
                }
            }
            if !domain_allowed {
                return false;
            }
        }

        // Skip common non-page resources
        let skip_extensions = [".jpg", ".jpeg", ".png", ".gif", ".css", ".js", ".pdf", ".zip"];
        let url_lower = url.to_lowercase();
        for ext in &skip_extensions {
            if url_lower.ends_with(ext) {
                return false;
            }
        }

        true
    }
}

// ===========================================================================
// 5. Reporter / Output Handler
// ===========================================================================

/// Report format
enum ReportFormat {
    Text,
    Json,
    Csv,
}

/// Results reporter
struct Reporter {
    format: ReportFormat,
}

impl Reporter {
    fn new(format: ReportFormat) -> Reporter / pure {
        Reporter { format }
    }

    /// Generate report from results
    fn report(&self, results: &[ScrapeResult], stats: &ScrapeStats) -> String / pure {
        match self.format {
            ReportFormat::Text => self.text_report(results, stats),
            ReportFormat::Json => self.json_report(results, stats),
            ReportFormat::Csv => self.csv_report(results),
        }
    }

    fn text_report(&self, results: &[ScrapeResult], stats: &ScrapeStats) -> String / pure {
        let mut report = String::new();

        report.push_str("=== Web Scraper Report ===\n\n");

        report.push_str("Statistics:\n");
        report.push_str(&format!("  Pages scraped: {}\n", stats.pages_scraped));
        report.push_str(&format!("  Pages failed: {}\n", stats.pages_failed));
        report.push_str(&format!("  Links found: {}\n", stats.links_found));
        report.push_str(&format!("  Bytes downloaded: {}\n", stats.bytes_downloaded));
        report.push_str(&format!("  Duration: {:.2}s\n", stats.duration_secs()));
        report.push_str(&format!("  Pages/second: {:.2}\n", stats.pages_per_second()));
        report.push_str("\n");

        report.push_str("Pages:\n");
        for result in results {
            let status = if result.error.is_some() { "FAILED" } else { "OK" };
            report.push_str(&format!("  [{}] {} (depth: {})\n",
                status, result.url, result.depth));

            if let Some(ref title) = result.title {
                report.push_str(&format!("       Title: {}\n", title));
            }

            if let Some(ref error) = result.error {
                report.push_str(&format!("       Error: {}\n", error));
            }

            report.push_str(&format!("       Links: {}\n", result.links.len()));
        }

        report
    }

    fn json_report(&self, results: &[ScrapeResult], stats: &ScrapeStats) -> String / pure {
        let mut json = String::new();

        json.push_str("{\n");
        json.push_str("  \"stats\": {\n");
        json.push_str(&format!("    \"pages_scraped\": {},\n", stats.pages_scraped));
        json.push_str(&format!("    \"pages_failed\": {},\n", stats.pages_failed));
        json.push_str(&format!("    \"links_found\": {},\n", stats.links_found));
        json.push_str(&format!("    \"duration_secs\": {:.2}\n", stats.duration_secs()));
        json.push_str("  },\n");

        json.push_str("  \"pages\": [\n");
        for (i, result) in results.iter().enumerate() {
            json.push_str("    {\n");
            json.push_str(&format!("      \"url\": \"{}\",\n", escape_json(&result.url)));
            json.push_str(&format!("      \"status_code\": {},\n", result.status_code));
            json.push_str(&format!("      \"depth\": {},\n", result.depth));

            if let Some(ref title) = result.title {
                json.push_str(&format!("      \"title\": \"{}\",\n", escape_json(title)));
            }

            if let Some(ref error) = result.error {
                json.push_str(&format!("      \"error\": \"{}\",\n", escape_json(error)));
            }

            json.push_str(&format!("      \"links_count\": {}\n", result.links.len()));
            json.push_str("    }");

            if i < results.len() - 1 {
                json.push(',');
            }
            json.push('\n');
        }
        json.push_str("  ]\n");
        json.push_str("}\n");

        json
    }

    fn csv_report(&self, results: &[ScrapeResult]) -> String / pure {
        let mut csv = String::new();

        csv.push_str("url,status_code,depth,title,links_count,error\n");
        for result in results {
            csv.push_str(&format!("\"{}\",{},{},\"{}\",{},\"{}\"\n",
                escape_csv(&result.url),
                result.status_code,
                result.depth,
                result.title.as_ref().map(|t| escape_csv(t)).unwrap_or_default(),
                result.links.len(),
                result.error.as_ref().map(|e| escape_csv(e)).unwrap_or_default()
            ));
        }

        csv
    }
}

/// Escape string for JSON
fn escape_json(s: &str) -> String / pure {
    let mut result = String::new();
    for c in s.chars() {
        match c {
            '"' => result.push_str("\\\""),
            '\\' => result.push_str("\\\\"),
            '\n' => result.push_str("\\n"),
            '\r' => result.push_str("\\r"),
            '\t' => result.push_str("\\t"),
            _ => result.push(c),
        }
    }
    result
}

/// Escape string for CSV
fn escape_csv(s: &str) -> String / pure {
    s.replace('"', "\"\"")
}

// ===========================================================================
// 6. Site Map Builder
// ===========================================================================

/// Represents a node in the site map
struct SiteMapNode {
    url: String,
    title: Option<String>,
    children: Vec<SiteMapNode>,
}

impl SiteMapNode {
    fn new(url: &str, title: Option<String>) -> SiteMapNode / pure {
        SiteMapNode {
            url: url.to_string(),
            title,
            children: Vec::new(),
        }
    }

    fn add_child(&mut self, child: SiteMapNode) / pure {
        self.children.push(child);
    }
}

/// Builds a site map from scrape results
fn build_site_map(results: &[ScrapeResult]) -> Option<SiteMapNode> / pure {
    if results.is_empty() {
        return None;
    }

    // Find root (depth 0)
    let root_result = results.iter().find(|r| r.depth == 0)?;

    let mut root = SiteMapNode::new(&root_result.url, root_result.title.clone());

    // Build tree recursively
    build_site_map_children(&mut root, results, 0);

    Some(root)
}

fn build_site_map_children(node: &mut SiteMapNode, results: &[ScrapeResult], depth: u32) / pure {
    // Find results at next depth that were linked from this node
    for result in results {
        if result.depth == depth + 1 {
            // Check if this was linked from current node
            let parent_result = results.iter().find(|r| r.url == node.url);
            if let Some(parent) = parent_result {
                if parent.links.iter().any(|l| l.url == result.url) {
                    let mut child = SiteMapNode::new(&result.url, result.title.clone());
                    build_site_map_children(&mut child, results, depth + 1);
                    node.add_child(child);
                }
            }
        }
    }
}

/// Print site map as tree
fn print_site_map(node: &SiteMapNode, indent: u32) -> String / pure {
    let mut output = String::new();

    let prefix = "  ".repeat(indent as usize);
    let title = node.title.as_ref().map(|t| format!(" ({})", t)).unwrap_or_default();
    output.push_str(&format!("{}- {}{}\n", prefix, node.url, title));

    for child in &node.children {
        output.push_str(&print_site_map(child, indent + 1));
    }

    output
}

// ===========================================================================
// 7. Main Function
// ===========================================================================

fn main() -> i32 / {Net, Async, IO} {
    println!("Blood Concurrent Web Scraper");
    println!("============================\n");

    // Configure scraper
    let config = ScraperConfig {
        max_depth: 2,
        max_pages: 20,
        concurrency: 3,
        rate_limit_ms: 500,
        user_agent: "Blood-WebScraper/1.0 (Educational Example)".to_string(),
        allowed_domains: Some(vec!["example.com".to_string()]),
        timeout_ms: 10000,
    };

    println!("Configuration:");
    println!("  Max depth: {}", config.max_depth);
    println!("  Max pages: {}", config.max_pages);
    println!("  Concurrency: {}", config.concurrency);
    println!("  Rate limit: {}ms", config.rate_limit_ms);
    println!();

    // Create scraper
    let scraper = WebScraper::new(config);

    // Record start time
    let start_time = perform Async.current_time_millis();

    // Start scraping
    println!("Starting scrape from http://example.com/...\n");
    let results = scraper.scrape("http://example.com/");

    // Record end time
    let end_time = perform Async.current_time_millis();

    // Calculate statistics
    let mut stats = ScrapeStats::new();
    stats.start_time = start_time;
    stats.end_time = end_time;

    for result in &results {
        if result.error.is_some() {
            stats.pages_failed += 1;
        } else {
            stats.pages_scraped += 1;
            stats.links_found += result.links.len() as u32;
        }
    }

    // Generate and print report
    let reporter = Reporter::new(ReportFormat::Text);
    let report = reporter.report(&results, &stats);
    println!("{}", report);

    // Print site map if we have results
    if let Some(site_map) = build_site_map(&results) {
        println!("\nSite Map:");
        println!("{}", print_site_map(&site_map, 0));
    }

    // Also show JSON output example
    println!("\n--- JSON Report ---");
    let json_reporter = Reporter::new(ReportFormat::Json);
    println!("{}", json_reporter.report(&results, &stats));

    println!("\nScraping complete!");
    0
}

// ===========================================================================
// 8. Tests
// ===========================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_link_is_relative() {
        let abs_link = Link::new("http://example.com/page", "Page");
        assert!(!abs_link.is_relative());

        let rel_link = Link::new("/about", "About");
        assert!(rel_link.is_relative());

        let rel_link2 = Link::new("page.html", "Page");
        assert!(rel_link2.is_relative());
    }

    #[test]
    fn test_link_resolve_absolute() {
        let link = Link::new("http://other.com/page", "Page");
        assert_eq!(link.resolve("http://example.com/"), "http://other.com/page");
    }

    #[test]
    fn test_link_resolve_root_relative() {
        let link = Link::new("/about", "About");
        assert_eq!(link.resolve("http://example.com/page/sub"), "http://example.com/about");
    }

    #[test]
    fn test_link_resolve_relative() {
        let link = Link::new("sibling.html", "Sibling");
        assert_eq!(link.resolve("http://example.com/dir/page.html"), "http://example.com/dir/sibling.html");
    }

    #[test]
    fn test_scrape_result_success() {
        let result = ScrapeResult::success(
            "http://example.com/",
            200,
            Some("Example".to_string()),
            Vec::new(),
            0
        );
        assert!(result.error.is_none());
        assert_eq!(result.status_code, 200);
    }

    #[test]
    fn test_scrape_result_failure() {
        let result = ScrapeResult::failure(
            "http://example.com/",
            "Connection timeout",
            0
        );
        assert!(result.error.is_some());
        assert_eq!(result.status_code, 0);
    }

    #[test]
    fn test_escape_json() {
        assert_eq!(escape_json("hello"), "hello");
        assert_eq!(escape_json("hello \"world\""), "hello \\\"world\\\"");
        assert_eq!(escape_json("line1\nline2"), "line1\\nline2");
    }

    #[test]
    fn test_escape_csv() {
        assert_eq!(escape_csv("hello"), "hello");
        assert_eq!(escape_csv("hello \"world\""), "hello \"\"world\"\"");
    }

    #[test]
    fn test_scrape_config_default() {
        let config = ScraperConfig::default();
        assert_eq!(config.max_depth, 3);
        assert_eq!(config.max_pages, 100);
        assert_eq!(config.concurrency, 5);
    }

    #[test]
    fn test_reporter_text_format() {
        let reporter = Reporter::new(ReportFormat::Text);
        let results = vec![
            ScrapeResult::success("http://example.com/", 200, Some("Example".to_string()), Vec::new(), 0),
        ];
        let stats = ScrapeStats {
            pages_scraped: 1,
            pages_failed: 0,
            links_found: 0,
            bytes_downloaded: 1000,
            start_time: 0,
            end_time: 1000,
        };
        let report = reporter.report(&results, &stats);
        assert!(report.contains("Pages scraped: 1"));
        assert!(report.contains("http://example.com/"));
    }

    #[test]
    fn test_site_map_node() {
        let mut root = SiteMapNode::new("http://example.com/", Some("Root".to_string()));
        let child = SiteMapNode::new("http://example.com/about", Some("About".to_string()));
        root.add_child(child);

        assert_eq!(root.children.len(), 1);
        assert_eq!(root.children[0].url, "http://example.com/about");
    }
}
